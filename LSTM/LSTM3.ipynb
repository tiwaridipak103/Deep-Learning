{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6RHrgOPi7kAq"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation \n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dense,Input,Conv1D,MaxPooling1D,Activation,Dropout,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import random as rn\n",
    "import keras\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YojPhuP234nG"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "I09OmQJF8TvZ",
    "outputId": "b58d2c42-3f98-436e-ddcf-bf2d3265bf3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mrs</td>\n",
       "      <td>in</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl_literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>154.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mr</td>\n",
       "      <td>fl</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>history_civics_health_sports</td>\n",
       "      <td>civics_government_teamsports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>299.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ms</td>\n",
       "      <td>az</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>health_sports</td>\n",
       "      <td>health_wellness_teamsports</td>\n",
       "      <td>Soccer Equipment for AWESOME Middle School Stu...</td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\\"True champions aren't always the ones th...</td>\n",
       "      <td>516.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix  ...   price\n",
       "0            mrs  ...  154.60\n",
       "1             mr  ...  299.00\n",
       "2             ms  ...  516.85\n",
       "\n",
       "[3 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LkkODKgb8eSD"
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns='project_is_approved')\n",
    "Y = data['project_is_approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9t0xzU5_-DzR"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_ZCUKos-eII",
    "outputId": "4032cdd2-9870-43bd-9249-1ae4d5a3faea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum essay length is 504\n",
      "maximum project_title length is 13\n",
      "maximum project_resource_summary length is 137\n"
     ]
    }
   ],
   "source": [
    "#max_review_length for essay data\n",
    "essay_length = []\n",
    "for i in X_train['essay']:\n",
    "  essay_length.append(len(i.split()))\n",
    "max_review_length_essay = max(essay_length)\n",
    "print(f'maximum essay length is {max_review_length_essay}')\n",
    "\n",
    "#max_review_length for project_title data\n",
    "title_length = []\n",
    "for i in X_train['project_title']:\n",
    "  title_length.append(len(i.split()))\n",
    "max_review_length_title = max(title_length)\n",
    "print(f'maximum project_title length is {max_review_length_title}')\n",
    "\n",
    "#max_review_length for project_title data\n",
    "summary_length = []\n",
    "for i in X_train['project_resource_summary']:\n",
    "  summary_length.append(len(i.split()))\n",
    "max_review_length_summary = max(summary_length)\n",
    "print(f'maximum project_resource_summary length is {max_review_length_summary}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IJ58E34Va-IT"
   },
   "outputs": [],
   "source": [
    "#pading for project_title\n",
    "unique_word = set()\n",
    "for i in X_train['project_title']:\n",
    "  for word in i.split():\n",
    "    unique_word.add(word)\n",
    "word_and_index_title = {word:j for j,word in enumerate(list(unique_word))}\n",
    "\n",
    "title_one_hot_encoding = []\n",
    "for i in X_train['project_title']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_title:\n",
    "      l.append(word_and_index_title[word])\n",
    "  title_one_hot_encoding.append(l)\n",
    "\n",
    "padded_project_title = sequence.pad_sequences(title_one_hot_encoding, maxlen=max_review_length_title, padding='post')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YUjOCSbud2Be"
   },
   "outputs": [],
   "source": [
    "#pading for project_resource_summary\n",
    "unique_word = set()\n",
    "for i in X_train['project_resource_summary']:\n",
    "  for word in i.split():\n",
    "    unique_word.add(word)\n",
    "word_and_index_summary = {word:j for j,word in enumerate(list(unique_word))}\n",
    "\n",
    "summary_one_hot_encoding = []\n",
    "for i in X_train['project_resource_summary']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_summary:\n",
    "      l.append(word_and_index_summary[word])\n",
    "  summary_one_hot_encoding.append(l)\n",
    "\n",
    "padded_project_summary = sequence.pad_sequences(summary_one_hot_encoding, maxlen=max_review_length_summary, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "esOJRkOoeRFR"
   },
   "outputs": [],
   "source": [
    "#list for teacher_prefix\n",
    "unique_word = set()\n",
    "for i in X_train['teacher_prefix']:\n",
    "  for word in i.split():\n",
    "    unique_word.add(word)\n",
    "word_and_index_teacher = {word:j for j,word in enumerate(list(unique_word))}\n",
    "\n",
    "X_train_teacher_prefix_one_hot_encoding = []\n",
    "for i in X_train['teacher_prefix']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_teacher:\n",
    "      l.append(word_and_index_teacher[word])\n",
    "  X_train_teacher_prefix_one_hot_encoding.append(l)\n",
    "\n",
    "X_test_teacher_prefix_one_hot_encoding = []\n",
    "for i in X_test['teacher_prefix']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_teacher:\n",
    "      l.append(word_and_index_teacher[word])\n",
    "  X_test_teacher_prefix_one_hot_encoding.append(l)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "koQZ4sTeewRX"
   },
   "outputs": [],
   "source": [
    "#list for school_state\n",
    "unique_word = set()\n",
    "for i in X_train['school_state']:\n",
    "  for word in i.split():\n",
    "    unique_word.add(word)\n",
    "word_and_index_school = {word:j for j,word in enumerate(list(unique_word))}\n",
    "\n",
    "X_train_school_state_one_hot_encoding = []\n",
    "for i in X_train['school_state']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_school:\n",
    "      l.append(word_and_index_school[word])\n",
    "  X_train_school_state_one_hot_encoding.append(l)\n",
    "\n",
    "\n",
    "X_test_school_state_one_hot_encoding = []\n",
    "for i in X_test['school_state']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_school:\n",
    "      l.append(word_and_index_school[word])\n",
    "  X_test_school_state_one_hot_encoding.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fXvYTvwUe5-V"
   },
   "outputs": [],
   "source": [
    "#list for project_grade_category\n",
    "unique_word = set()\n",
    "for i in X_train['project_grade_category']:\n",
    "  for word in i.split():\n",
    "    unique_word.add(word)\n",
    "word_and_index_grade = {word:j for j,word in enumerate(list(unique_word))}\n",
    "\n",
    "X_train_project_grade_category_one_hot_encoding = []\n",
    "for i in X_train['project_grade_category']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_grade:\n",
    "      l.append(word_and_index_grade[word])\n",
    "  X_train_project_grade_category_one_hot_encoding.append(l)\n",
    "\n",
    "\n",
    "X_test_project_grade_category_one_hot_encoding = []\n",
    "for i in X_test['project_grade_category']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_grade:\n",
    "      l.append(word_and_index_grade[word])\n",
    "  X_test_project_grade_category_one_hot_encoding.append(l)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qxmzu-2vfDRO"
   },
   "outputs": [],
   "source": [
    "#list for project_subject_categories\n",
    "unique_word = set()\n",
    "for i in X_train['project_subject_categories']:\n",
    "  for word in i.split():\n",
    "    unique_word.add(word)\n",
    "word_and_index_subject = {word:j for j,word in enumerate(list(unique_word))}\n",
    "\n",
    "X_train_project_subject_categories_one_hot_encoding = []\n",
    "for i in X_train['project_subject_categories']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_subject:\n",
    "      l.append(word_and_index_subject[word])\n",
    "  X_train_project_subject_categories_one_hot_encoding.append(l)\n",
    "\n",
    "X_test_project_subject_categories_one_hot_encoding = []\n",
    "for j in X_test['project_subject_categories']:\n",
    "  l = []\n",
    "  for word in j.split():\n",
    "    if word in word_and_index_subject:\n",
    "      l.append(word_and_index_subject[word])\n",
    "    else:\n",
    "      l.append(0)\n",
    "  X_test_project_subject_categories_one_hot_encoding.append(l) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CshwkD-7fT9j"
   },
   "outputs": [],
   "source": [
    "#list for project_subject_subcategories\n",
    "unique_word = set()\n",
    "for i in X_train['project_subject_subcategories']:\n",
    "  for word in i.split():\n",
    "    unique_word.add(word)\n",
    "word_and_index_sub = {word:j for j,word in enumerate(list(unique_word))}\n",
    "\n",
    "X_train_project_subject_subcategories_one_hot_encoding = []\n",
    "for i in X_train['project_subject_subcategories']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_sub:\n",
    "      l.append(word_and_index_sub[word])\n",
    "  X_train_project_subject_subcategories_one_hot_encoding.append(l)\n",
    "\n",
    "X_test_project_subject_subcategories_one_hot_encoding = []\n",
    "for i in X_test['project_subject_subcategories']:\n",
    "  l = []\n",
    "  for word in i.split():\n",
    "    if word in word_and_index_sub:\n",
    "      l.append(word_and_index_sub[word])\n",
    "    else:\n",
    "      l.append(0)  \n",
    "  X_test_project_subject_subcategories_one_hot_encoding.append(l)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KKUQhjSxoYlz"
   },
   "outputs": [],
   "source": [
    "tnop = []\n",
    "price = []\n",
    "for i , row in X_train.iterrows():\n",
    "  tnop.append(int(row['teacher_number_of_previously_posted_projects'])) \n",
    "  price.append(int(row['price']))\n",
    "\n",
    "X_train_numeracal_matrix = np.zeros((len(X_train),2))\n",
    "for i in range(len(X_train)):\n",
    "  X_train_numeracal_matrix[i][0] = tnop[i]\n",
    "  X_train_numeracal_matrix[i][1] = price[i]\n",
    "\n",
    "\n",
    "norm_X_train_numeracal_matrix = normalize(X_train_numeracal_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdGSsH2wnZRW",
    "outputId": "0eac0bad-c024-4d08-b177-8ccaaf52ab4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "tnop = []\n",
    "price = []\n",
    "for i , row in X_test.iterrows():\n",
    "  tnop.append(int(row['teacher_number_of_previously_posted_projects'])) \n",
    "  price.append(int(row['price']))\n",
    "\n",
    "X_test_numeracal_matrix = np.zeros((len(X_test),2))\n",
    "for i in range(len(X_test)):\n",
    "  X_test_numeracal_matrix[i][0] = tnop[i]\n",
    "  X_test_numeracal_matrix[i][1] = price[i]\n",
    "\n",
    "norm_X_test_numeracal_matrix = normalize(X_test_numeracal_matrix, axis=1)  \n",
    "print(len(tnop),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elrp2kL9D_Yw",
    "outputId": "07bee96b-2471-43f0-8e37-79a0722eb128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "tnop = []\n",
    "price = []\n",
    "for i , row in X_test.iterrows():\n",
    "  if row['price'] is not None:\n",
    "    tnop.append(row['price']) \n",
    "print(len(tnop))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvCSZw_2DrRf",
    "outputId": "0a10ec4c-9ae5-42a4-df2b-0d4bba0615d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "tnop = []\n",
    "price = []\n",
    "for i , row in X_test.iterrows():\n",
    "  if row['teacher_number_of_previously_posted_projects'] is not None:\n",
    "    tnop.append(row['teacher_number_of_previously_posted_projects']) \n",
    "  price.append(row['price'])\n",
    "\n",
    "X_test_numeracal_matrix = np.zeros((len(X_test),2))\n",
    "for i in range(len(X_test)):\n",
    "  X_test_numeracal_matrix[i][0] = tnop[i]\n",
    "  X_test_numeracal_matrix[i][1] = price[i]\n",
    "\n",
    "norm_X_test_numeracal_matrix = normalize(X_test_numeracal_matrix, axis=1)  \n",
    "print(len(tnop),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4dziM0-z_tKL"
   },
   "outputs": [],
   "source": [
    "Y_train = tf.keras.utils.to_categorical(y_train, 2) \n",
    "Y_test = tf.keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HCRR-LYCNRP",
    "outputId": "b4cbdba0-fcc4-43df-c161-c0be99742dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 51510 word vectors.\n",
      "Loaded 46337 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "# prepare tokenizer\n",
    "t = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "t.fit_on_texts(X_train['essay'])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "X_train_encoded_essay = t.texts_to_sequences(X_train['essay'])\n",
    "\n",
    "X_test_encoded_essay = t.texts_to_sequences(X_test['essay'])\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = max_review_length_essay\n",
    "X_train_padded_essay = sequence.pad_sequences(X_train_encoded_essay, maxlen=max_length, padding='post')\n",
    "X_test_padded_essay = sequence.pad_sequences(X_test_encoded_essay, maxlen=max_length, padding='post')\n",
    "\n",
    "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "# make sure you have the glove_vectors file\n",
    "with open('glove_vectors', 'rb') as f:\n",
    " model = pickle.load(f)\n",
    " embeddings_index = dict(zip(model.keys(),model.values()))\n",
    " f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "#create a weight matrix for words in training docs\n",
    "#each word is of 300 dimension\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Loaded %s word vectors.' % len(embedding_matrix))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNtcgC4DnrgZ"
   },
   "source": [
    "## MODEL :- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1M2U-yTOdnBv"
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
    "## Varibles will also set to some value from before session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "embedding_vecor_length = 30\n",
    "\n",
    "\n",
    "\n",
    "inputs_essay = Input(shape=(max_review_length_essay,),)\n",
    "inputs_school_state = Input(shape=(1,),)\n",
    "inputs_project_grade_category = Input(shape=(1,), )\n",
    "inputs_project_subject_categories = Input(shape=(1,), )\n",
    "inputs_project_subject_subcategories = Input(shape=(1,), )\n",
    "inputs_teacher_prefix = Input(shape=(1,), )\n",
    "inputs_numerical = Input(shape=(2,), )\n",
    "\n",
    "lstm = LSTM(units = 100 ,activation=\"tanh\",kernel_initializer=tf.keras.initializers.he_uniform(seed=0),return_sequences=True)\n",
    "\n",
    "\n",
    "essay_embedding = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_review_length_essay, trainable=True)(inputs_essay)\n",
    "lstm_output   = lstm(essay_embedding)\n",
    "glo_avg = tf.keras.layers.GlobalAveragePooling1D()(lstm_output)\n",
    "flatten_essay = Flatten()(glo_avg)\n",
    "\n",
    "school_state_embedding = Embedding(len(word_and_index_school), 2, input_length=1)(inputs_school_state)\n",
    "flatten_school_state = Flatten()(school_state_embedding)\n",
    "\n",
    "project_grade_category_embedding = Embedding(len(word_and_index_grade), 2, input_length=1)(inputs_project_grade_category)\n",
    "flatten_project_grade_category = Flatten()(project_grade_category_embedding)\n",
    "\n",
    "project_subject_categories_embedding = Embedding(len(word_and_index_subject), 2, input_length=1)(inputs_project_subject_categories)\n",
    "flatten_project_subject_categories = Flatten()(project_subject_categories_embedding)\n",
    "\n",
    "project_subject_subcategories_embedding = Embedding(len(word_and_index_sub), 2, input_length=1)(inputs_project_subject_subcategories)\n",
    "flatten_project_subject_subcategories = Flatten()(project_subject_subcategories_embedding)\n",
    "\n",
    "teacher_prefix_embedding = Embedding(len(word_and_index_teacher), 2, input_length=1)(inputs_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(teacher_prefix_embedding)\n",
    "\n",
    "dense_numerical = Dense(1, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(inputs_numerical)\n",
    "\n",
    "combine_con1 = keras.layers.concatenate([flatten_essay, flatten_school_state,flatten_project_grade_category,\n",
    "                                         flatten_project_subject_categories,flatten_project_subject_subcategories,\n",
    "                                         flatten_teacher_prefix,dense_numerical])\n",
    "\n",
    "\n",
    "dense1 = Dense(128, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(combine_con1)\n",
    "\n",
    "drop_out1 = Dropout(0.1)(dense1)\n",
    "\n",
    "dense2 = Dense(64, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(drop_out1)\n",
    "\n",
    "drop_out2 = Dropout(0.1)(dense2)\n",
    "\n",
    "dense3 = Dense(8, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(drop_out2)\n",
    "\n",
    "Out = Dense(units=2,activation='softmax',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(dense3)\n",
    "\n",
    "#Creating a model\n",
    "model = Model(inputs=[inputs_essay, inputs_school_state,inputs_project_grade_category,inputs_project_subject_categories,\n",
    "                      inputs_project_subject_subcategories,inputs_teacher_prefix,inputs_numerical],outputs=Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgKIe3P3KN_T",
    "outputId": "bb2cb3ba-a08e-4e3c-bc05-ace3cfacc02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 504)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 504, 300)     13901100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 504, 100)     160400      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 100)          0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 2)         102         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 2)         8           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 2)         98          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 2)         748         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 2)         10          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2)            0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            3           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 111)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          14336       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            520         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            18          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,085,599\n",
      "Trainable params: 14,085,599\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "G31RsDLIgWYp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "class f1_score_and_auc_Callback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def  on_train_begin(self,logs={}):\n",
    "      self.auc_score=[]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "      y_pred_=self.model.predict(x_test)\n",
    "      y_true=Y_test\n",
    "      Auc_score  = roc_auc_score(y_true, y_pred_,average='samples')\n",
    "\n",
    "      \n",
    "      self.auc_score.append(Auc_score)\n",
    "      print(\" auc score :\",Auc_score)\n",
    "\n",
    "metrics=f1_score_and_auc_Callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "G6NwUcctFaIw"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.RMSprop()\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTCKKg6oUcVe",
    "outputId": "8dbedab0-f48f-44a7-9aff-bbf707e4e2c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 45000 45000 45000 45000 45000 45000\n",
      "15000 15000 15000 15000 15000 15000 15000\n"
     ]
    }
   ],
   "source": [
    "#np.asarray(x).astype('float32').\n",
    "#all train features\n",
    "e = np.asarray(X_train_padded_essay).astype(np.float32)\n",
    "s = np.asarray(X_train_school_state_one_hot_encoding).astype(np.float32)\n",
    "p = np.asarray(X_train_project_grade_category_one_hot_encoding).astype(np.float32)\n",
    "ps = np.asarray(X_train_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
    "psu = np.asarray(X_train_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
    "t = np.asarray(X_train_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
    "n = np.asarray(norm_X_train_numeracal_matrix).astype(np.float32)\n",
    "\n",
    "x_train = [e,s,p,ps,psu,t,n]\n",
    "print(len(e),len(s),len(p),len(ps),len(psu),len(t),len(n))\n",
    "\n",
    "\n",
    "#all test features\n",
    "e1 = np.asarray(X_test_padded_essay).astype(np.float32)\n",
    "s1 = np.asarray(X_test_school_state_one_hot_encoding).astype(np.float32)\n",
    "p1 = np.asarray(X_test_project_grade_category_one_hot_encoding).astype(np.float32)\n",
    "ps1 = np.asarray(X_test_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
    "psu1 = np.asarray(X_test_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
    "t1 = np.asarray(X_test_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
    "n1 = np.asarray(norm_X_test_numeracal_matrix).astype(np.float32)\n",
    "\n",
    "x_test = [e1,s1,p1,ps1,psu1,t1,n1]\n",
    "print(len(e1),len(s1),len(p1),len(ps1),len(psu1),len(t1),len(n1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipf2Mp279wcw",
    "outputId": "6e06c1b4-3a48-4590-edf6-f32ebba0dede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 146s 80ms/step - loss: 0.4144 - val_loss: 0.3745\n",
      " auc score : 0.8502\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 112s 80ms/step - loss: 0.3676 - val_loss: 0.3707\n",
      " auc score : 0.8486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f25ac3da710>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,Y_train,epochs=2,validation_data=(x_test ,Y_test),callbacks=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2F1kkploGBI"
   },
   "source": [
    "## MODEL :- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "8kXLIqA2M2qN",
    "outputId": "5e8c2ddb-e95b-4020-df14-be8a8a910965"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn  as sns\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train['essay'])\n",
    "word_dic = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))\n",
    "\n",
    "df = pd.DataFrame({'word':vectorizer.get_feature_names(),'idf':vectorizer.idf_})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.title(\"Box Plot of idf for important feature\",fontsize=15)\n",
    "plt.grid()\n",
    "ax=sns.boxplot(y='idf', data=df)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper right', ncol=2, bbox_to_anchor=(.75, 0.98))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oC-XziB9FFk",
    "outputId": "170c8eea-cbc2-43a9-ed00-ca3533b02e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42424\n"
     ]
    }
   ],
   "source": [
    "#we are sorting idf value in descending order of top 50 idf values\n",
    "vocab = sorted(word_dic.items(), key = lambda d:(d[1], d[0]))\n",
    "vocab = { i[0]:i[1] for i in vocab}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "SHgHT8nmj1ex"
   },
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in word_dic.items():\n",
    "  if i[1] >9 and i[1] <11:\n",
    "    word_list.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ue1BzAVvr4Q",
    "outputId": "a13d5157-bf52-4514-8fac-50b34b06a474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15667\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2uvZ7hkDax0"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "X_train_essay_list = []\n",
    "for i, row in X_train.iterrows():\n",
    "  l = []\n",
    "  for word in row['essay'].split():\n",
    "    if word in word_list:\n",
    "      l.append(word)\n",
    "  X_train_essay_list.append(' '.join(l)) \n",
    "  \n",
    "\n",
    "X_test_essay_list = []\n",
    "for i, row in X_test.iterrows():\n",
    "  l = []\n",
    "  for word in row['essay'].split():\n",
    "    if word in word_list:\n",
    "      l.append(word)\n",
    "  X_test_essay_list.append(' '.join(l))   \n",
    "\n",
    "df1 = pd.DataFrame({'essay':X_train_essay_list})\n",
    "df1.to_pickle('X_train_essay_imp_word')\n",
    "\n",
    "df2 = pd.DataFrame({'essay':X_test_essay_list})\n",
    "df2.to_pickle('X_test_essay_imp_word') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "FJueIrShdubq"
   },
   "outputs": [],
   "source": [
    "X_train_essay = pd.read_pickle('X_train_essay_imp_word')\n",
    "X_test_essay = pd.read_pickle('X_test_essay_imp_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DYZ9OYJeCTx",
    "outputId": "f997a2c0-bf29-4b29-dac4-33496c2a616c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocal size 10834\n",
      "Loaded 51510 word vectors.\n",
      "Loaded 10834 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "# prepare tokenizer\n",
    "t = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "t.fit_on_texts(X_train_essay['essay'])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(f'vocal size {vocab_size}')\n",
    "# integer encode the documents\n",
    "X_train_encoded_essay = t.texts_to_sequences(X_train_essay['essay'])\n",
    "\n",
    "X_test_encoded_essay = t.texts_to_sequences(X_test_essay['essay'])\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = max_review_length_essay\n",
    "X_train_padded_essay = sequence.pad_sequences(X_train_encoded_essay, maxlen=max_length, padding='post')\n",
    "X_test_padded_essay = sequence.pad_sequences(X_test_encoded_essay, maxlen=max_length, padding='post')\n",
    "\n",
    "\n",
    "with open('glove_vectors', 'rb') as f:\n",
    " model = pickle.load(f)\n",
    " embeddings_index = dict(zip(model.keys(),model.values()))\n",
    " f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "#create a weight matrix for words in training docs\n",
    "#each word is of 300 dimension\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Loaded %s word vectors.' % len(embedding_matrix))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "tANnHWPk7MtX"
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
    "## Varibles will also set to some value from before session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "embedding_vecor_length = 30\n",
    "\n",
    "\n",
    "inputs_essay = Input(shape=(max_review_length_essay,),)\n",
    "inputs_school_state = Input(shape=(1,), )\n",
    "inputs_project_grade_category = Input(shape=(1,), )\n",
    "inputs_project_subject_categories = Input(shape=(1,), )\n",
    "inputs_project_subject_subcategories = Input(shape=(1,),)\n",
    "inputs_teacher_prefix = Input(shape=(1,), )\n",
    "inputs_numerical = Input(shape=(2,), )\n",
    "\n",
    "lstm = LSTM(units = 100 ,activation=\"tanh\",return_sequences=True)\n",
    "\n",
    "\n",
    "essay_embedding = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_review_length_essay, trainable=True)(inputs_essay)\n",
    "lstm_output   = lstm(essay_embedding)\n",
    "glo_avg = tf.keras.layers.GlobalAveragePooling1D()(lstm_output)\n",
    "flatten_essay = Flatten()(glo_avg)\n",
    "\n",
    "school_state_embedding = Embedding(len(word_and_index_school), 2, input_length=1)(inputs_school_state)\n",
    "flatten_school_state = Flatten()(school_state_embedding)\n",
    "\n",
    "project_grade_category_embedding = Embedding(len(word_and_index_grade), 2, input_length=1)(inputs_project_grade_category)\n",
    "flatten_project_grade_category = Flatten()(project_grade_category_embedding)\n",
    "\n",
    "project_subject_categories_embedding = Embedding(len(word_and_index_subject), 2, input_length=1)(inputs_project_subject_categories)\n",
    "flatten_project_subject_categories = Flatten()(project_subject_categories_embedding)\n",
    "\n",
    "project_subject_subcategories_embedding = Embedding(len(word_and_index_sub), 2, input_length=1)(inputs_project_subject_subcategories)\n",
    "flatten_project_subject_subcategories = Flatten()(project_subject_subcategories_embedding)\n",
    "\n",
    "teacher_prefix_embedding = Embedding(len(word_and_index_teacher), 2, input_length=1)(inputs_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(teacher_prefix_embedding)\n",
    "\n",
    "dense_numerical = Dense(1, activation='relu')(inputs_numerical)\n",
    "\n",
    "combine_con1 = keras.layers.concatenate([flatten_essay, flatten_school_state,flatten_project_grade_category,\n",
    "                                         flatten_project_subject_categories,flatten_project_subject_subcategories,\n",
    "                                         flatten_teacher_prefix,dense_numerical])\n",
    "\n",
    "\n",
    "dense1 = Dense(128, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(combine_con1)\n",
    "\n",
    "drop_out1 = Dropout(0.1)(dense1)\n",
    "\n",
    "dense2 = Dense(64, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(drop_out1)\n",
    "\n",
    "drop_out2 = Dropout(0.1)(dense2)\n",
    "\n",
    "dense3 = Dense(8, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(drop_out2)\n",
    "\n",
    "Out = Dense(units=2,activation='softmax',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(dense3)\n",
    "\n",
    "#Creating a model\n",
    "model = Model(inputs=[inputs_essay, inputs_school_state,inputs_project_grade_category,inputs_project_subject_categories,\n",
    "                      inputs_project_subject_subcategories,inputs_teacher_prefix,inputs_numerical],outputs=Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sC0QL-NRyRpt",
    "outputId": "18f9e759-816f-40f7-b5e9-dda562121a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 504)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 504, 300)     3250200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 504, 100)     160400      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 100)          0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 2)         102         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 2)         8           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 2)         98          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 2)         748         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 2)         10          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2)            0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            3           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 111)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          14336       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            520         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            18          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,434,699\n",
      "Trainable params: 3,434,699\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "e2dx4EDfYytl"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.RMSprop()\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzDyZvqihhDX",
    "outputId": "7c28db51-8a61-424e-c691-7d3fc708e367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 45000 45000 45000 45000 45000 45000\n",
      "15000 15000 15000 15000 15000 15000 15000\n"
     ]
    }
   ],
   "source": [
    "#np.asarray(x).astype('float32').\n",
    "#all train features\n",
    "e = np.asarray(X_train_padded_essay).astype(np.float32)\n",
    "s = np.asarray(X_train_school_state_one_hot_encoding).astype(np.float32)\n",
    "p = np.asarray(X_train_project_grade_category_one_hot_encoding).astype(np.float32)\n",
    "ps = np.asarray(X_train_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
    "psu = np.asarray(X_train_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
    "t = np.asarray(X_train_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
    "n = np.asarray(norm_X_train_numeracal_matrix).astype(np.float32)\n",
    "\n",
    "x_train = [e,s,p,ps,psu,t,n]\n",
    "print(len(e),len(s),len(p),len(ps),len(psu),len(t),len(n))\n",
    "\n",
    "#all test features\n",
    "e1 = np.asarray(X_test_padded_essay).astype(np.float32)\n",
    "s1 = np.asarray(X_test_school_state_one_hot_encoding).astype(np.float32)\n",
    "p1 = np.asarray(X_test_project_grade_category_one_hot_encoding).astype(np.float32)\n",
    "ps1 = np.asarray(X_test_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
    "psu1 = np.asarray(X_test_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
    "t1 = np.asarray(X_test_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
    "n1 = np.asarray(norm_X_test_numeracal_matrix).astype(np.float32)\n",
    "\n",
    "x_test = [e1,s1,p1,ps1,psu1,t1,n1]\n",
    "print(len(e1),len(s1),len(p1),len(ps1),len(psu1),len(t1),len(n1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiwICu6ojDZE",
    "outputId": "fe3c4822-dc6f-43ad-e605-e5e70c48a6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 68s 47ms/step - loss: 0.4390 - val_loss: 0.4243\n",
      " auc score : 0.8476\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 65s 46ms/step - loss: 0.4195 - val_loss: 0.4213\n",
      " auc score : 0.8476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f254e45d7d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,Y_train,epochs=2,validation_data=(x_test ,Y_test),callbacks=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgYwxpZErXos"
   },
   "source": [
    "## MODEL :- 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljTBe9Ighebs",
    "outputId": "5361a8ba-fbc1-4a1e-9dab-aeb1583fd371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 45000\n",
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "#all train features\n",
    "e = np.asarray(X_train_padded_essay).astype(np.float32)\n",
    "s = np.asarray(X_train_school_state_one_hot_encoding).astype(np.float32)\n",
    "p = np.asarray(X_train_project_grade_category_one_hot_encoding).astype(np.float32)\n",
    "ps = np.asarray(X_train_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
    "psu = np.asarray(X_train_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
    "t = np.asarray(X_train_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
    "n = np.asarray(norm_X_train_numeracal_matrix).astype(np.float32)\n",
    "\n",
    "X_train_other_than_text_data  = np.concatenate((s,p,ps,psu,t,n) ,axis = 1)\n",
    "\n",
    "x_train = [e,X_train_other_than_text_data]\n",
    "print(len(e),len(X_train_other_than_text_data))\n",
    "\n",
    "\n",
    "#all test features\n",
    "e1 = np.asarray(X_test_padded_essay).astype(np.float32)\n",
    "s1 = np.asarray(X_test_school_state_one_hot_encoding).astype(np.float32)\n",
    "p1 = np.asarray(X_test_project_grade_category_one_hot_encoding).astype(np.float32)\n",
    "ps1 = np.asarray(X_test_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
    "psu1 = np.asarray(X_test_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
    "t1 = np.asarray(X_test_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
    "n1 = np.asarray(norm_X_test_numeracal_matrix).astype(np.float32)\n",
    "\n",
    "X_test_other_than_text_data  = np.concatenate((s1,p1,ps1,psu1,t1,n1) ,axis = 1)\n",
    "\n",
    "x_test = [e1,X_test_other_than_text_data]\n",
    "print(len(e1),len(X_test_other_than_text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "dFIRo-iepCyN"
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
    "## Varibles will also set to some value from before session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "embedding_vecor_length = 30\n",
    "\n",
    "inputs_essay = Input(shape=(max_review_length_essay,))\n",
    "inputs_other_than_text_data = Input(shape=(X_train_other_than_text_data.shape[1],1,),dtype=float)\n",
    "\n",
    "\n",
    "lstm = LSTM(units = 100 ,activation=\"tanh\",kernel_initializer=tf.keras.initializers.he_uniform(seed=0),return_sequences=True)\n",
    "\n",
    "\n",
    "essay_embedding = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_review_length_essay, trainable=True)(inputs_essay)\n",
    "lstm_output   = lstm(essay_embedding)\n",
    "glo_avg = tf.keras.layers.GlobalAveragePooling1D()(lstm_output)\n",
    "flatten_essay = Flatten()(glo_avg)\n",
    "\n",
    "con1 = Conv1D(filters=10, kernel_size=2, activation='relu')(inputs_other_than_text_data)\n",
    "\n",
    "con2 = Conv1D(filters=5, kernel_size=2, activation='relu')(con1)\n",
    "\n",
    "flatten_remaining = Flatten()(con2)\n",
    "\n",
    "combine_con1 = keras.layers.concatenate([flatten_essay, flatten_remaining])\n",
    "\n",
    "\n",
    "dense1 = Dense(128, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(combine_con1)\n",
    "\n",
    "drop_out1 = Dropout(0.1)(dense1)\n",
    "\n",
    "dense2 = Dense(64, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(drop_out1)\n",
    "\n",
    "drop_out2 = Dropout(0.1)(dense2)\n",
    "\n",
    "dense3 = Dense(8, activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(drop_out2)\n",
    "\n",
    "Out = Dense(units=2,activation='softmax',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(dense3)\n",
    "\n",
    "#Creating a model\n",
    "model = Model(inputs=[inputs_essay, inputs_other_than_text_data],outputs=Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38dhTJI9sNlj",
    "outputId": "e08b2690-a8ab-4fb4-a52b-0489f08e2fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 504)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 504, 300)     3250200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 504, 100)     160400      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 6, 10)        30          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 100)          0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5, 5)         105         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 25)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 125)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          16128       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8)            520         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            18          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,435,657\n",
      "Trainable params: 3,435,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "5W00HaQJ2N6N"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.RMSprop()\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PZWyz9-pMWR",
    "outputId": "13c392ad-cb54-42db-9876-f41f17fd28b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 65s 45ms/step - loss: 0.4626 - val_loss: 0.4286\n",
      " auc score : 0.8476\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 63s 45ms/step - loss: 0.4315 - val_loss: 0.4249\n",
      " auc score : 0.8476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f25ace6c450>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,Y_train,epochs=2,validation_data=(x_test ,Y_test),callbacks=metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
