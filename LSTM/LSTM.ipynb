{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H7KfgE72-DS",
        "outputId": "a3b405ba-4c87-4d88-80c2-3f3181360eb4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('content')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at content; to attempt to forcibly remount, call drive.mount(\"content\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W55fk7Ib3NeW",
        "outputId": "0d5d719d-0342-45d7-8e6a-4c313c545d99"
      },
      "source": [
        "cd /content/content/Shareddrives/HK ROBOT/applied_ai_asignments/lstm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/content/Shareddrives/HK ROBOT/applied_ai_asignments/lstm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StlpzsHr3lZx",
        "outputId": "93b535b2-6967-4fc1-ef43-c53f705fc7cd"
      },
      "source": [
        "cd /content/content/Shareddrives/HK ROBOT/applied_ai_asignments/lstm/LSTM Assignment"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/content/Shareddrives/HK ROBOT/applied_ai_asignments/lstm/LSTM Assignment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iodp4le03xTh",
        "outputId": "5eec39e8-e346-4e25-ff31-579854b6ab5b"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1_Reference_EDA.ipynb             preprocessed_data.csv\n",
            " 2_Reference_Preprocessing.ipynb   resources.csv\n",
            " essay_imp_word                    tensorboard.ipynb\n",
            " glove_vectors                     train_data.csv\n",
            "'LSTM - Assignment.ipynb'          X_test_essay_imp_word\n",
            " preprocessed_data2.csv            X_train_essay_imp_word\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RHrgOPi7kAq"
      },
      "source": [
        "import shutil\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation \n",
        "from tensorflow.keras import initializers\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Dense,Input,Conv1D,MaxPooling1D,Activation,Dropout,Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import random as rn\n",
        "import keras\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YojPhuP234nG"
      },
      "source": [
        "data = pd.read_csv('preprocessed_data2.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "I09OmQJF8TvZ",
        "outputId": "23001f38-aeba-4818-b12f-e316abac8c24"
      },
      "source": [
        "data.head(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>teacher_prefix</th>\n",
              "      <th>school_state</th>\n",
              "      <th>project_grade_category</th>\n",
              "      <th>project_subject_categories</th>\n",
              "      <th>project_subject_subcategories</th>\n",
              "      <th>project_title</th>\n",
              "      <th>project_resource_summary</th>\n",
              "      <th>teacher_number_of_previously_posted_projects</th>\n",
              "      <th>project_is_approved</th>\n",
              "      <th>essay</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mrs</td>\n",
              "      <td>in</td>\n",
              "      <td>grades_prek_2</td>\n",
              "      <td>literacy_language</td>\n",
              "      <td>esl_literacy</td>\n",
              "      <td>Educational Support for English Learners at Home</td>\n",
              "      <td>My students need opportunities to practice beg...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>My students are English learners that are work...</td>\n",
              "      <td>154.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mr</td>\n",
              "      <td>fl</td>\n",
              "      <td>grades_6_8</td>\n",
              "      <td>history_civics_health_sports</td>\n",
              "      <td>civics_government_teamsports</td>\n",
              "      <td>Wanted: Projector for Hungry Learners</td>\n",
              "      <td>My students need a projector to help with view...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Our students arrive to our school eager to lea...</td>\n",
              "      <td>299.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ms</td>\n",
              "      <td>az</td>\n",
              "      <td>grades_6_8</td>\n",
              "      <td>health_sports</td>\n",
              "      <td>health_wellness_teamsports</td>\n",
              "      <td>Soccer Equipment for AWESOME Middle School Stu...</td>\n",
              "      <td>My students need shine guards, athletic socks,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>\\r\\n\\\"True champions aren't always the ones th...</td>\n",
              "      <td>516.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  teacher_prefix  ...   price\n",
              "0            mrs  ...  154.60\n",
              "1             mr  ...  299.00\n",
              "2             ms  ...  516.85\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkkODKgb8eSD"
      },
      "source": [
        "X = data.drop(columns='project_is_approved')\n",
        "Y = data['project_is_approved']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t0xzU5_-DzR"
      },
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, stratify=Y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_ZCUKos-eII",
        "outputId": "e1334d50-9aa6-4d03-e2f6-147d80eb0800"
      },
      "source": [
        "#max_review_length for essay data\n",
        "essay_length = []\n",
        "for i in X_train['essay']:\n",
        "  essay_length.append(len(i.split()))\n",
        "max_review_length_essay = max(essay_length)\n",
        "print(f'maximum essay length is {max_review_length_essay}')\n",
        "\n",
        "#max_review_length for project_title data\n",
        "title_length = []\n",
        "for i in X_train['project_title']:\n",
        "  title_length.append(len(i.split()))\n",
        "max_review_length_title = max(title_length)\n",
        "print(f'maximum project_title length is {max_review_length_title}')\n",
        "\n",
        "#max_review_length for project_title data\n",
        "summary_length = []\n",
        "for i in X_train['project_resource_summary']:\n",
        "  summary_length.append(len(i.split()))\n",
        "max_review_length_summary = max(summary_length)\n",
        "print(f'maximum project_resource_summary length is {max_review_length_summary}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum essay length is 504\n",
            "maximum project_title length is 13\n",
            "maximum project_resource_summary length is 137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ58E34Va-IT"
      },
      "source": [
        "#pading for project_title\n",
        "unique_word = set()\n",
        "for i in X_train['project_title']:\n",
        "  for word in i.split():\n",
        "    unique_word.add(word)\n",
        "word_and_index_title = {word:j for j,word in enumerate(list(unique_word))}\n",
        "\n",
        "title_one_hot_encoding = []\n",
        "for i in X_train['project_title']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_title:\n",
        "      l.append(word_and_index_title[word])\n",
        "  title_one_hot_encoding.append(l)\n",
        "\n",
        "padded_project_title = sequence.pad_sequences(title_one_hot_encoding, maxlen=max_review_length_title, padding='post')      \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUjOCSbud2Be"
      },
      "source": [
        "#pading for project_resource_summary\n",
        "unique_word = set()\n",
        "for i in X_train['project_resource_summary']:\n",
        "  for word in i.split():\n",
        "    unique_word.add(word)\n",
        "word_and_index_summary = {word:j for j,word in enumerate(list(unique_word))}\n",
        "\n",
        "summary_one_hot_encoding = []\n",
        "for i in X_train['project_resource_summary']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_summary:\n",
        "      l.append(word_and_index_summary[word])\n",
        "  summary_one_hot_encoding.append(l)\n",
        "\n",
        "padded_project_summary = sequence.pad_sequences(summary_one_hot_encoding, maxlen=max_review_length_summary, padding='post') "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esOJRkOoeRFR"
      },
      "source": [
        "#list for teacher_prefix\n",
        "unique_word = set()\n",
        "for i in X_train['teacher_prefix']:\n",
        "  for word in i.split():\n",
        "    unique_word.add(word)\n",
        "word_and_index_teacher = {word:j for j,word in enumerate(list(unique_word))}\n",
        "\n",
        "X_train_teacher_prefix_one_hot_encoding = []\n",
        "for i in X_train['teacher_prefix']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_teacher:\n",
        "      l.append(word_and_index_teacher[word])\n",
        "  X_train_teacher_prefix_one_hot_encoding.append(l)\n",
        "\n",
        "X_test_teacher_prefix_one_hot_encoding = []\n",
        "for i in X_test['teacher_prefix']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_teacher:\n",
        "      l.append(word_and_index_teacher[word])\n",
        "  X_test_teacher_prefix_one_hot_encoding.append(l)  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koQZ4sTeewRX"
      },
      "source": [
        "#list for school_state\n",
        "unique_word = set()\n",
        "for i in X_train['school_state']:\n",
        "  for word in i.split():\n",
        "    unique_word.add(word)\n",
        "word_and_index_school = {word:j for j,word in enumerate(list(unique_word))}\n",
        "\n",
        "X_train_school_state_one_hot_encoding = []\n",
        "for i in X_train['school_state']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_school:\n",
        "      l.append(word_and_index_school[word])\n",
        "  X_train_school_state_one_hot_encoding.append(l)\n",
        "\n",
        "\n",
        "X_test_school_state_one_hot_encoding = []\n",
        "for i in X_test['school_state']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_school:\n",
        "      l.append(word_and_index_school[word])\n",
        "  X_test_school_state_one_hot_encoding.append(l)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXvYTvwUe5-V"
      },
      "source": [
        "#list for project_grade_category\n",
        "unique_word = set()\n",
        "for i in X_train['project_grade_category']:\n",
        "  for word in i.split():\n",
        "    unique_word.add(word)\n",
        "word_and_index_grade = {word:j for j,word in enumerate(list(unique_word))}\n",
        "\n",
        "X_train_project_grade_category_one_hot_encoding = []\n",
        "for i in X_train['project_grade_category']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_grade:\n",
        "      l.append(word_and_index_grade[word])\n",
        "  X_train_project_grade_category_one_hot_encoding.append(l)\n",
        "\n",
        "\n",
        "X_test_project_grade_category_one_hot_encoding = []\n",
        "for i in X_test['project_grade_category']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_grade:\n",
        "      l.append(word_and_index_grade[word])\n",
        "  X_test_project_grade_category_one_hot_encoding.append(l)  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxmzu-2vfDRO"
      },
      "source": [
        "#list for project_subject_categories\n",
        "unique_word = set()\n",
        "for i in X_train['project_subject_categories']:\n",
        "  for word in i.split():\n",
        "    unique_word.add(word)\n",
        "word_and_index_subject = {word:j for j,word in enumerate(list(unique_word))}\n",
        "\n",
        "X_train_project_subject_categories_one_hot_encoding = []\n",
        "for i in X_train['project_subject_categories']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_subject:\n",
        "      l.append(word_and_index_subject[word])\n",
        "  X_train_project_subject_categories_one_hot_encoding.append(l)\n",
        "\n",
        "X_test_project_subject_categories_one_hot_encoding = []\n",
        "for j in X_test['project_subject_categories']:\n",
        "  l = []\n",
        "  for word in j.split():\n",
        "    if word in word_and_index_subject:\n",
        "      l.append(word_and_index_subject[word])\n",
        "    else:\n",
        "      l.append(0)\n",
        "  X_test_project_subject_categories_one_hot_encoding.append(l) \n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CshwkD-7fT9j"
      },
      "source": [
        "#list for project_subject_subcategories\n",
        "unique_word = set()\n",
        "for i in X_train['project_subject_subcategories']:\n",
        "  for word in i.split():\n",
        "    unique_word.add(word)\n",
        "word_and_index_sub = {word:j for j,word in enumerate(list(unique_word))}\n",
        "\n",
        "X_train_project_subject_subcategories_one_hot_encoding = []\n",
        "for i in X_train['project_subject_subcategories']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_sub:\n",
        "      l.append(word_and_index_sub[word])\n",
        "  X_train_project_subject_subcategories_one_hot_encoding.append(l)\n",
        "\n",
        "X_test_project_subject_subcategories_one_hot_encoding = []\n",
        "for i in X_test['project_subject_subcategories']:\n",
        "  l = []\n",
        "  for word in i.split():\n",
        "    if word in word_and_index_sub:\n",
        "      l.append(word_and_index_sub[word])\n",
        "    else:\n",
        "      l.append(0)  \n",
        "  X_test_project_subject_subcategories_one_hot_encoding.append(l)  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKUQhjSxoYlz"
      },
      "source": [
        "tnop = []\n",
        "price = []\n",
        "for i , row in X_train.iterrows():\n",
        "  tnop.append(int(row['teacher_number_of_previously_posted_projects'])) \n",
        "  price.append(int(row['price']))\n",
        "\n",
        "X_train_numeracal_matrix = np.zeros((len(X_train),2))\n",
        "for i in range(len(X_train)):\n",
        "  X_train_numeracal_matrix[i][0] = tnop[i]\n",
        "  X_train_numeracal_matrix[i][1] = price[i]\n",
        "\n",
        "\n",
        "norm_X_train_numeracal_matrix = normalize(X_train_numeracal_matrix, axis=1)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdGSsH2wnZRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7580d1-cb38-4290-cadf-baea3b93e32d"
      },
      "source": [
        "tnop = []\n",
        "price = []\n",
        "for i , row in X_test.iterrows():\n",
        "  tnop.append(int(row['teacher_number_of_previously_posted_projects'])) \n",
        "  price.append(int(row['price']))\n",
        "\n",
        "X_test_numeracal_matrix = np.zeros((len(X_test),2))\n",
        "for i in range(len(X_test)):\n",
        "  X_test_numeracal_matrix[i][0] = tnop[i]\n",
        "  X_test_numeracal_matrix[i][1] = price[i]\n",
        "\n",
        "norm_X_test_numeracal_matrix = normalize(X_test_numeracal_matrix, axis=1)  \n",
        "print(len(tnop),len(price))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15000 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elrp2kL9D_Yw",
        "outputId": "32bf7631-6089-4e95-c36e-4f98f2c97efc"
      },
      "source": [
        "tnop = []\n",
        "price = []\n",
        "for i , row in X_test.iterrows():\n",
        "  if row['price'] is not None:\n",
        "    tnop.append(row['price']) \n",
        "print(len(tnop))    "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvCSZw_2DrRf"
      },
      "source": [
        "tnop = []\n",
        "price = []\n",
        "for i , row in X_test.iterrows():\n",
        "  if row['teacher_number_of_previously_posted_projects'] is not None:\n",
        "    tnop.append(row['teacher_number_of_previously_posted_projects']) \n",
        "  price.append(row['price'])\n",
        "\n",
        "X_test_numeracal_matrix = np.zeros((len(X_test),2))\n",
        "for i in range(len(X_test)):\n",
        "  X_test_numeracal_matrix[i][0] = tnop[i]\n",
        "  X_test_numeracal_matrix[i][1] = price[i]\n",
        "\n",
        "norm_X_test_numeracal_matrix = normalize(X_test_numeracal_matrix, axis=1)  \n",
        "print(len(tnop),len(price))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dziM0-z_tKL"
      },
      "source": [
        "Y_train = tf.keras.utils.to_categorical(y_train, 2) \n",
        "Y_test = tf.keras.utils.to_categorical(y_test, 2)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HCRR-LYCNRP",
        "outputId": "ce49c133-ecfc-43c9-e552-37e60bf16971"
      },
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "# prepare tokenizer\n",
        "t = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
        "t.fit_on_texts(X_train['essay'])\n",
        "vocab_size = len(t.word_index) + 1\n",
        "# integer encode the documents\n",
        "X_train_encoded_essay = t.texts_to_sequences(X_train['essay'])\n",
        "\n",
        "X_test_encoded_essay = t.texts_to_sequences(X_test['essay'])\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = max_review_length_essay\n",
        "X_train_padded_essay = sequence.pad_sequences(X_train_encoded_essay, maxlen=max_length, padding='post')\n",
        "X_test_padded_essay = sequence.pad_sequences(X_test_encoded_essay, maxlen=max_length, padding='post')\n",
        "\n",
        "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
        "# make sure you have the glove_vectors file\n",
        "with open('glove_vectors', 'rb') as f:\n",
        " model = pickle.load(f)\n",
        " embeddings_index = dict(zip(model.keys(),model.values()))\n",
        " f.close()\n",
        "\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "#create a weight matrix for words in training docs\n",
        "#each word is of 300 dimension\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "print('Loaded %s word vectors.' % len(embedding_matrix))    \n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 51510 word vectors.\n",
            "Loaded 46477 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o61fHsqrrOIo"
      },
      "source": [
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
        "## Varibles will also set to some value from before session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## Set the random seed values to regenerate the model.\n",
        "np.random.seed(0)\n",
        "rn.seed(0)\n",
        "\n",
        "embedding_vecor_length = 30\n",
        "\n",
        "#Input layer\n",
        "#input_layer = Input(shape=(156,256,3),name='Input_Layer')\n",
        "\n",
        "inputs_essay = Input(shape=(max_review_length_essay,), dtype='int32')\n",
        "inputs_school_state = Input(shape=(1,), dtype='int32')\n",
        "inputs_project_grade_category = Input(shape=(1,), dtype='int32')\n",
        "inputs_project_subject_categories = Input(shape=(1,), dtype='int32')\n",
        "inputs_project_subject_subcategories = Input(shape=(1,), dtype='int32')\n",
        "inputs_teacher_prefix = Input(shape=(1,), dtype='int32')\n",
        "inputs_numerical = Input(shape=(2,), dtype='int32')\n",
        "\n",
        "\n",
        "essay_embedding = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_review_length_essay, trainable=False)(inputs_essay)\n",
        "essay_lstm = LSTM(100)(essay_embedding)\n",
        "flatten_essay = Flatten()(essay_lstm)\n",
        "\n",
        "school_state_embedding = Embedding(len(word_and_index_school), 1, input_length=1)(inputs_school_state)\n",
        "flatten_school_state = Flatten()(school_state_embedding)\n",
        "\n",
        "project_grade_category_embedding = Embedding(len(word_and_index_grade), 1, input_length=1)(inputs_project_grade_category)\n",
        "flatten_project_grade_category = Flatten()(project_grade_category_embedding)\n",
        "\n",
        "project_subject_categories_embedding = Embedding(len(word_and_index_subject), 1, input_length=1)(inputs_project_subject_categories)\n",
        "flatten_project_subject_categories = Flatten()(project_subject_categories_embedding)\n",
        "\n",
        "project_subject_subcategories_embedding = Embedding(len(word_and_index_sub), 1, input_length=1)(inputs_project_subject_subcategories)\n",
        "flatten_project_subject_subcategories = Flatten()(project_subject_subcategories_embedding)\n",
        "\n",
        "teacher_prefix_embedding = Embedding(len(word_and_index_teacher), 1, input_length=1)(inputs_teacher_prefix)\n",
        "flatten_teacher_prefix = Flatten()(teacher_prefix_embedding)\n",
        "\n",
        "dense_numerical = Dense(1, activation='relu')(inputs_numerical)\n",
        "\n",
        "combine_con1 = keras.layers.concatenate([flatten_essay, flatten_school_state,flatten_project_grade_category,\n",
        "                                         flatten_project_subject_categories,flatten_project_subject_subcategories,\n",
        "                                         flatten_teacher_prefix,dense_numerical])\n",
        "\n",
        "\n",
        "dense1 = Dense(100, activation='relu')(combine_con1)\n",
        "\n",
        "drop_out1 = Dropout(0.1)(dense1)\n",
        "\n",
        "dense2 = Dense(20, activation='relu')(drop_out1)\n",
        "\n",
        "drop_out2 = Dropout(0.1)(dense2)\n",
        "\n",
        "dense3 = Dense(5, activation='relu')(drop_out2)\n",
        "\n",
        "Out = Dense(units=2,activation='softmax')(dense3)\n",
        "\n",
        "#Creating a model\n",
        "model = Model(inputs=[inputs_essay, inputs_school_state,inputs_project_grade_category,inputs_project_subject_categories,\n",
        "                      inputs_project_subject_subcategories,inputs_teacher_prefix,inputs_numerical],outputs=Out)\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgKIe3P3KN_T",
        "outputId": "9fe4e4fe-b0ef-4547-c56f-bce65df04b48"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 504)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 504, 300)     13943100    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 100)          160400      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 1)         51          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 1)         4           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 1)         50          input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 1)         380         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 1, 1)         5           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100)          0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1)            0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 1)            0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 1)            0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 1)            0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1)            0           embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 106)          0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "                                                                 flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          10700       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 100)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 20)           2020        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 20)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 5)            105         dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 2)            12          dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 14,116,830\n",
            "Trainable params: 173,730\n",
            "Non-trainable params: 13,943,100\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqRqeTVdSxaz"
      },
      "source": [
        "#https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nzd2lF9Yyaq"
      },
      "source": [
        "#https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc1():\n",
        "  y_pred=model.predict(x_test).round()\n",
        "  y_true=Y_test\n",
        "  return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HIWTNOVZg3k"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "class f1_score_and_auc_Callback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def  on_train_begin(self,logs={}):\n",
        "      self.f1_micro=[]\n",
        "      self.auc_score=[]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      y_pred=self.model.predict(x_test).round()\n",
        "      y_true=Y_test\n",
        "      score=f1_score(y_true, y_pred, average='micro')\n",
        "      Auc_score  = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "      self.f1_micro.append(score)\n",
        "      self.auc_score.append(Auc_score)\n",
        "      print(\" F1 micro :\",score)\n",
        "      print(\" auc score :\",Auc_score)\n",
        "\n",
        "metrics=f1_score_and_auc_Callback()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ggXxX2_9FS"
      },
      "source": [
        "#compiling \n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTCKKg6oUcVe",
        "outputId": "dd3ad256-b922-46a6-9626-5e4ef682ccfd"
      },
      "source": [
        "#np.asarray(x).astype('float32').\n",
        "#all train features\n",
        "e = np.asarray(X_train_padded_essay).astype(np.float32)\n",
        "s = np.asarray(X_train_school_state_one_hot_encoding).astype(np.float32)\n",
        "p = np.asarray(X_train_project_grade_category_one_hot_encoding).astype(np.float32)\n",
        "ps = np.asarray(X_train_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
        "psu = np.asarray(X_train_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
        "t = np.asarray(X_train_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
        "n = np.asarray(norm_X_train_numeracal_matrix).astype(np.float32)\n",
        "\n",
        "x_train = [e,s,p,ps,psu,t,n]\n",
        "print(len(e),len(s),len(p),len(ps),len(psu),len(t),len(n))\n",
        "\n",
        "\n",
        "#all test features\n",
        "e1 = np.asarray(X_test_padded_essay).astype(np.float32)\n",
        "s1 = np.asarray(X_test_school_state_one_hot_encoding).astype(np.float32)\n",
        "p1 = np.asarray(X_test_project_grade_category_one_hot_encoding).astype(np.float32)\n",
        "ps1 = np.asarray(X_test_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
        "psu1 = np.asarray(X_test_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
        "t1 = np.asarray(X_test_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
        "n1 = np.asarray(norm_X_test_numeracal_matrix).astype(np.float32)\n",
        "\n",
        "x_test = [e1,s1,p1,ps1,psu1,t1,n1]\n",
        "print(len(e1),len(s1),len(p1),len(ps1),len(psu1),len(t1),len(n1))\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45000 45000 45000 45000 45000 45000 45000\n",
            "15000 15000 15000 15000 15000 15000 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmtlj5mOxq21",
        "outputId": "5e7b5fc4-8a3d-4fbe-8539-f14bf624ba35"
      },
      "source": [
        "model.fit(x_train,Y_train,epochs=2,validation_data=(x_test ,Y_test),callbacks=metrics)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1407/1407 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.8476 F1 micro : 0.8476\n",
            " auc score : 0.5\n",
            "1407/1407 [==============================] - 1013s 720ms/step - loss: 0.4272 - accuracy: 0.8476 - val_loss: 0.4270 - val_accuracy: 0.8476\n",
            "Epoch 2/2\n",
            "1407/1407 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8476 F1 micro : 0.8476\n",
            " auc score : 0.5\n",
            "1407/1407 [==============================] - 1056s 751ms/step - loss: 0.4271 - accuracy: 0.8476 - val_loss: 0.4269 - val_accuracy: 0.8476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffb0286a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "YBG4026ZjAAZ",
        "outputId": "87230100-0575-4718-fc28-17ed1a5bed00"
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn  as sns\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(X_train['essay'])\n",
        "word_dic = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))\n",
        "\n",
        "df = pd.DataFrame({'word':vectorizer.get_feature_names(),'idf':vectorizer.idf_})\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.title(\"Box Plot of idf for important feature\",fontsize=15)\n",
        "plt.grid()\n",
        "ax=sns.boxplot(y='idf', data=df)\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, labels, loc='upper right', ncol=2, bbox_to_anchor=(.75, 0.98))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oC-XziB9FFk",
        "outputId": "7f0f49c2-e53e-418f-8cc1-d6a26423c61f"
      },
      "source": [
        "#we are sorting idf value in descending order of top 50 idf values\n",
        "vocab = sorted(word_dic.items(), key = lambda d:(d[1], d[0]))\n",
        "vocab = { i[0]:i[1] for i in vocab}\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHgHT8nmj1ex"
      },
      "source": [
        "word_list = []\n",
        "for i in word_dic.items():\n",
        "  if i[1] >9 and i[1] <11:\n",
        "    word_list.append(i[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ue1BzAVvr4Q",
        "outputId": "b35eec9d-5a56-44ad-a497-efe121369abe"
      },
      "source": [
        "print(len(word_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm-N0pbgwOLM"
      },
      "source": [
        "from tqdm import tqdm\n",
        "X_train_essay_list = []\n",
        "for i, row in X_train.iterrows():\n",
        "  l = []\n",
        "  for word in row['essay'].split():\n",
        "    if word in word_list:\n",
        "      l.append(word)\n",
        "  essay_list.append(' '.join(l)) \n",
        "  \n",
        "\n",
        "from tqdm import tqdm\n",
        "X_test_essay_list = []\n",
        "for i, row in X_test.iterrows():\n",
        "  l = []\n",
        "  for word in row['essay'].split():\n",
        "    if word in word_list:\n",
        "      l.append(word)\n",
        "  X_test_essay_list.append(' '.join(l))  \n",
        "\n",
        "X_test_essay_list = []\n",
        "for i, row in X_test.iterrows():\n",
        "  l = []\n",
        "  for word in row['essay'].split():\n",
        "    if word in word_list:\n",
        "      l.append(word)\n",
        "  X_test_essay_list.append(' '.join(l))   \n",
        "\n",
        "df1 = pd.DataFrame({'essay':essay_list})\n",
        "df1.to_pickle('X_train_essay_imp_word')\n",
        "\n",
        "df2 = pd.DataFrame({'essay':X_test_essay_list})\n",
        "df2.to_pickle('X_test_essay_imp_word')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJueIrShdubq"
      },
      "source": [
        "X_train_essay = pd.read_pickle('X_train_essay_imp_word')\n",
        "X_test_essay = pd.read_pickle('X_test_essay_imp_word')"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DYZ9OYJeCTx",
        "outputId": "5d8c0d03-2bc0-4a84-c97c-71979093342c"
      },
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "# prepare tokenizer\n",
        "t = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
        "t.fit_on_texts(X_train_essay['essay'])\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(f'vocal size {vocab_size}')\n",
        "# integer encode the documents\n",
        "X_train_encoded_essay = t.texts_to_sequences(X_train_essay['essay'])\n",
        "\n",
        "X_test_encoded_essay = t.texts_to_sequences(X_test_essay['essay'])\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = max_review_length_essay\n",
        "X_train_padded_essay = sequence.pad_sequences(X_train_encoded_essay, maxlen=max_length, padding='post')\n",
        "X_test_padded_essay = sequence.pad_sequences(X_test_encoded_essay, maxlen=max_length, padding='post')\n",
        "\n",
        "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
        "# make sure you have the glove_vectors file\n",
        "with open('glove_vectors', 'rb') as f:\n",
        " model = pickle.load(f)\n",
        " embeddings_index = dict(zip(model.keys(),model.values()))\n",
        " f.close()\n",
        "\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "#create a weight matrix for words in training docs\n",
        "#each word is of 300 dimension\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "print('Loaded %s word vectors.' % len(embedding_matrix))    "
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocal size 10683\n",
            "Loaded 51510 word vectors.\n",
            "Loaded 10683 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfJPYFUBebxO"
      },
      "source": [
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
        "## Varibles will also set to some value from before session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## Set the random seed values to regenerate the model.\n",
        "np.random.seed(0)\n",
        "rn.seed(0)\n",
        "\n",
        "embedding_vecor_length = 30\n",
        "\n",
        "#Input layer\n",
        "#input_layer = Input(shape=(156,256,3),name='Input_Layer')\n",
        "\n",
        "inputs_essay = Input(shape=(max_review_length_essay,), dtype='int32')\n",
        "inputs_school_state = Input(shape=(1,), dtype='int32')\n",
        "inputs_project_grade_category = Input(shape=(1,), dtype='int32')\n",
        "inputs_project_subject_categories = Input(shape=(1,), dtype='int32')\n",
        "inputs_project_subject_subcategories = Input(shape=(1,), dtype='int32')\n",
        "inputs_teacher_prefix = Input(shape=(1,), dtype='int32')\n",
        "inputs_numerical = Input(shape=(2,), dtype='int32')\n",
        "\n",
        "\n",
        "essay_embedding = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_review_length_essay, trainable=False)(inputs_essay)\n",
        "essay_lstm = LSTM(100)(essay_embedding)\n",
        "flatten_essay = Flatten()(essay_lstm)\n",
        "\n",
        "school_state_embedding = Embedding(len(word_and_index_school), 1, input_length=1)(inputs_school_state)\n",
        "flatten_school_state = Flatten()(school_state_embedding)\n",
        "\n",
        "project_grade_category_embedding = Embedding(len(word_and_index_grade), 1, input_length=1)(inputs_project_grade_category)\n",
        "flatten_project_grade_category = Flatten()(project_grade_category_embedding)\n",
        "\n",
        "project_subject_categories_embedding = Embedding(len(word_and_index_subject), 1, input_length=1)(inputs_project_subject_categories)\n",
        "flatten_project_subject_categories = Flatten()(project_subject_categories_embedding)\n",
        "\n",
        "project_subject_subcategories_embedding = Embedding(len(word_and_index_sub), 1, input_length=1)(inputs_project_subject_subcategories)\n",
        "flatten_project_subject_subcategories = Flatten()(project_subject_subcategories_embedding)\n",
        "\n",
        "teacher_prefix_embedding = Embedding(len(word_and_index_teacher), 1, input_length=1)(inputs_teacher_prefix)\n",
        "flatten_teacher_prefix = Flatten()(teacher_prefix_embedding)\n",
        "\n",
        "dense_numerical = Dense(1, activation='relu')(inputs_numerical)\n",
        "\n",
        "combine_con1 = keras.layers.concatenate([flatten_essay, flatten_school_state,flatten_project_grade_category,\n",
        "                                         flatten_project_subject_categories,flatten_project_subject_subcategories,\n",
        "                                         flatten_teacher_prefix,dense_numerical])\n",
        "\n",
        "\n",
        "dense1 = Dense(100, activation='relu')(combine_con1)\n",
        "\n",
        "drop_out1 = Dropout(0.1)(dense1)\n",
        "\n",
        "dense2 = Dense(20, activation='relu')(drop_out1)\n",
        "\n",
        "drop_out2 = Dropout(0.1)(dense2)\n",
        "\n",
        "dense3 = Dense(5, activation='relu')(drop_out2)\n",
        "\n",
        "Out = Dense(units=2,activation='softmax')(dense3)\n",
        "\n",
        "#Creating a model\n",
        "model = Model(inputs=[inputs_essay, inputs_school_state,inputs_project_grade_category,inputs_project_subject_categories,\n",
        "                      inputs_project_subject_subcategories,inputs_teacher_prefix,inputs_numerical],outputs=Out)\n",
        "\n",
        "#compiling \n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzDyZvqihhDX",
        "outputId": "fbb8dbfb-5c7f-481a-d7a6-f0a20d6d2cb8"
      },
      "source": [
        "#np.asarray(x).astype('float32').\n",
        "#all train features\n",
        "e = np.asarray(X_train_padded_essay).astype(np.float32)\n",
        "s = np.asarray(X_train_school_state_one_hot_encoding).astype(np.float32)\n",
        "p = np.asarray(X_train_project_grade_category_one_hot_encoding).astype(np.float32)\n",
        "ps = np.asarray(X_train_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
        "psu = np.asarray(X_train_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
        "t = np.asarray(X_train_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
        "n = np.asarray(norm_X_train_numeracal_matrix).astype(np.float32)\n",
        "\n",
        "x_train = [e,s,p,ps,psu,t,n]\n",
        "print(len(e),len(s),len(p),len(ps),len(psu),len(t),len(n))\n",
        "\n",
        "#all test features\n",
        "e1 = np.asarray(X_test_padded_essay).astype(np.float32)\n",
        "s1 = np.asarray(X_test_school_state_one_hot_encoding).astype(np.float32)\n",
        "p1 = np.asarray(X_test_project_grade_category_one_hot_encoding).astype(np.float32)\n",
        "ps1 = np.asarray(X_test_project_subject_categories_one_hot_encoding).astype(np.float32)\n",
        "psu1 = np.asarray(X_test_project_subject_subcategories_one_hot_encoding).astype(np.float32)\n",
        "t1 = np.asarray(X_test_teacher_prefix_one_hot_encoding).astype(np.float32)\n",
        "n1 = np.asarray(norm_X_test_numeracal_matrix).astype(np.float32)\n",
        "\n",
        "x_test = [e1,s1,p1,ps1,psu1,t1,n1]\n",
        "print(len(e1),len(s1),len(p1),len(ps1),len(psu1),len(t1),len(n1))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45000 45000 45000 45000 45000 45000 45000\n",
            "15000 15000 15000 15000 15000 15000 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kllfcqYuoWV-",
        "outputId": "955a2252-f5cd-4772-bcce-2132e79d291a"
      },
      "source": [
        "model.fit(x_train,Y_train,epochs=2,validation_data=(x_test ,Y_test),callbacks=metrics)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1407/1407 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8471 F1 micro : 0.8476\n",
            " auc score : 0.5\n",
            "1407/1407 [==============================] - 1022s 726ms/step - loss: 0.4283 - accuracy: 0.8471 - val_loss: 0.4276 - val_accuracy: 0.8476\n",
            "Epoch 2/2\n",
            "1407/1407 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.8476 F1 micro : 0.8476\n",
            " auc score : 0.5\n",
            "1407/1407 [==============================] - 1029s 731ms/step - loss: 0.4248 - accuracy: 0.8476 - val_loss: 0.4264 - val_accuracy: 0.8476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffb156040f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    }
  ]
}